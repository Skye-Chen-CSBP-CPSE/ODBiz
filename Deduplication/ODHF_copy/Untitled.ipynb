{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T16:24:39.788830Z",
     "iopub.status.busy": "2022-09-06T16:24:39.788224Z",
     "iopub.status.idle": "2022-09-06T16:24:42.042193Z",
     "shell.execute_reply": "2022-09-06T16:24:42.041044Z",
     "shell.execute_reply.started": "2022-09-06T16:24:39.788740Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/astroid/node_classes.py:94: DeprecationWarning: The 'astroid.node_classes' module is deprecated and will be replaced by 'astroid.nodes' in astroid 3.0.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'importlib_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m strip_accents, AddressClean, haversine\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrecordlinkage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#from recordlinkage.preprocessing import clean\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/rl_helper/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl_helper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m envhelper,VDisplay\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl_helper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fps\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl_helper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExperimentManager\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/rl_helper/fps.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_vector_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncVectorEnv\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msync_vector_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SyncVectorEnv\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorEnv, VectorEnvWrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     Env,\n\u001b[1;32m      9\u001b[0m     Wrapper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     RewardWrapper,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Space\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make, spec, register\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vector\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_env_plugins \u001b[38;5;28;01mas\u001b[39;00m _load_env_plugins\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make, register, registry, spec\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Hook to load plugins from entry points\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/registration.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_checker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PassiveEnvChecker\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib_metadata\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmetadata\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmetadata\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'importlib_metadata'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "This script looks for potential duplicates in a database\n",
    "\n",
    "We are assuming a set of standard columns,\n",
    "Name, Address, StreetNumber, StreetName, City, PostalCode, Province, Latitude, Longitude\n",
    "\n",
    "There are two sections, an initial Preprocessing step, and then the comparison script\n",
    "\n",
    "With minor modification, this can also be used for record linkage between two files.\n",
    "\n",
    "I) Preprocessing:\n",
    "    0. Read in JSON 'source' file that contains\n",
    "        i. file name of database\n",
    "        ii. mappings from database column names to standard names\n",
    "        iii. a short dictionary of terms to replace in the Name field to improve\n",
    "            potential matches (e.g., 'ch' for 'centre hospitalier')\n",
    "    1. Read in database (columns as values from json)\n",
    "    2. Strip all accents from all text fields\n",
    "    3. Process Address and Street Name fields to standardise street types\n",
    "    4. run recordlinkage's \"clean\" function to remove extra whitespace and any\n",
    "        remaining non-ascii characters, and anything in parentheses\n",
    "    5. Standardise PostalCode\n",
    "        \n",
    "II) Record Linkage:\n",
    "    Use RecordLinkageToolkit to perform comparisons and create index pairs:\n",
    "        Criteria:\n",
    "                Province - Block (consider only matches where equal)\n",
    "                Name - Damerau-Levenshtein, qgram\n",
    "                Address - Damerau-Levenshtein, Cosine\n",
    "                StreetNumber - Exact\n",
    "                StreetName - Damerau-Levenshtein, Cosine\n",
    "                City - Damerau-Levenshtein\n",
    "                PostalCode - Exact\n",
    "                Latitude/Longitude - Distance\n",
    "\n",
    "    The result is a Pandas multiindex object, which we then use to create a file\n",
    "    where every line contains the two objects being compared. \n",
    "    \n",
    "    This output file will be fed to a machine learning script for classification.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from rl_helper import strip_accents, AddressClean, haversine\n",
    "import recordlinkage as rl\n",
    "#from recordlinkage.preprocessing import clean\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "Read in source file, data file, and rename data file columns\n",
    "'''\n",
    "sourcefile = '/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/1NAICS_Test.json'\n",
    "with open(sourcefile) as source_f:\n",
    "    Source = json.load(source_f)\n",
    "    \n",
    "\n",
    "df = pd.read_csv(Source[\"filename\"],\n",
    "                 encoding=Source[\"encoding\"],\n",
    "                 index_col=Source[\"index\"])\n",
    "\n",
    "#df = pd.read_csv( '/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/1NAICS_Test.json', encoding=Source[\"encoding\"], index_col=Source[\"index\"])\n",
    "\n",
    "print('I. Preprocessing - renaming columns, removing accents, and making string replacements.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reduce database to only the columns we use for comparisons\n",
    "df = df[Source[\"column_map\"].values()]\n",
    "column_map = {val: key for key, val in Source[\"column_map\"].items()}\n",
    "df = df.rename(columns = column_map)\n",
    "\n",
    "# remove accents\n",
    "\n",
    "text_cols = ['Name','Address','StreetName','City']\n",
    "\n",
    "for col in text_cols:\n",
    "    df.loc[~df[col].isnull(),col]=df.loc[~df[col].isnull(),col].apply(strip_accents)\n",
    "    \n",
    "#make names lowercase\n",
    "df['Name'] = df['Name'].apply(str)\n",
    "df['Name'] = df['Name'].str.lower()\n",
    "\n",
    "df['Address'] = df['Address'].apply(str)\n",
    "df['Address'] = df['Address'].str.lower()\n",
    "\n",
    "df['StreetName'] = df['StreetName'].apply(str)\n",
    "df['StreetName'] = df['StreetName'].str.lower()\n",
    "\n",
    "\n",
    "''' \n",
    "#apply text swaps in the Name column    \n",
    "for swap in Source[\"text_map\"]:\n",
    "    start = r'\\b'+re.escape(swap[0])+r'\\b'\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(start,swap[1], regex=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#standardise addresses using \"AddressClean\" function in the rl_helper module\n",
    "\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "#remove periods, apostrophes, commas, and hypens in the Name and address columns\n",
    "\n",
    "r_list = [r\".\", r\",\", r\"'\", r\"-\"]\n",
    "\n",
    "for r in r_list:\n",
    "\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(r, ' ', regex=False)\n",
    "    df[\"Address\"] = df[\"Address\"].str.replace(r, ' ', regex=False)\n",
    "\n",
    "#remove excess whitespace\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(r\" +\", \" \", regex=True)\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(r\" +\", \" \", regex=True)\n",
    "\n",
    "#standardise postal codes - just remove empty space and make sure it's all lower case\n",
    "\n",
    "df.loc[~df.PostalCode.isnull(), 'PostalCode'] = df.loc[~df.PostalCode.isnull(), 'PostalCode'].str.replace(' ', '', regex=True).str.lower()\n",
    "\n",
    "#create an extra temporary Name column with an additional level of cleaning\n",
    "\n",
    "df['NameClean'] = clean(df[\"Name\"])\n",
    "\n",
    "#Some records have street number and street name, but no address field filled\n",
    "\n",
    "df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'Address']\\\n",
    "    =clean(df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetNumber']+' '+\\\n",
    "           df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetName']+' '+\\\n",
    "        df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'City'])\n",
    "\n",
    "#df.to_csv('test.csv')\n",
    "\n",
    "''' \n",
    "\n",
    "\n",
    "r\"\"\"\n",
    "II. Record Linkage\n",
    "\n",
    "\n",
    "This is the section that uses the record linkage package to determine candidate pairs,\n",
    "which will be evaluated separately.\n",
    "\"\"\"\n",
    "print('II. Record linkage - Now creating multiindex and performing comparisons')\n",
    "\n",
    "indexer = rl.Index()\n",
    "indexer.block('PostalCode')\n",
    "candidate_links = indexer.index(df)\n",
    "\n",
    "print('Computing metrics for {} candidate pairs'.format(len(candidate_links)))\n",
    "\n",
    "# likely to be a lot of records to match, so split into chunks\n",
    "n = math.ceil(len(candidate_links) / 1E5)\n",
    "chunks = rl.index_split(candidate_links, n)\n",
    "\n",
    "# Comparison step\n",
    "results = []\n",
    "\n",
    "# n_jobs specifies number of cores for running in parallel\n",
    "compare = rl.Compare(n_jobs=4)\n",
    "\n",
    "compare.exact('StreetNumber', 'StreetNumber', label='StrNum_Match')\n",
    "compare.exact('PostalCode', 'PostalCode', label='PC_Match')\n",
    "compare.exact('FileName', 'FileName', label='File_Match')\n",
    "compare.exact('Type', 'Type', label='Type_Match')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='Addr_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='Addr_CS')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='StrName_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='StrName_CS')\n",
    "compare.string('City', 'City', method='damerau_levenshtein', label='City_DL')\n",
    "compare.string('Name', 'Name', method='damerau_levenshtein', label='Name_DL')\n",
    "compare.string('Name', 'Name', method='cosine', label='Name_CS')\n",
    "compare.string('Name', 'Name', method='qgram', label='Name_Q')\n",
    "compare.string(\"NameClean\", \"NameClean\", method='damerau_levenshtein', label=\"CleanName_DL\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    i += 1\n",
    "    print('processing chunk {} of {}'.format(i,n))\n",
    "\n",
    "    features = compare.compute(chunk, df)\n",
    "\n",
    "    #reduce comparison matrix to entries where the name score is reasonably high\n",
    "\n",
    "    cutoff = 0\n",
    "    features = features.loc[features.Name_CS > cutoff]\n",
    "    results.append(features)\n",
    "f = pd.concat(results)\n",
    "print('Score cut-off of {} reduced candidate pairs to {}'.format(cutoff, len(f)))\n",
    "\n",
    "f['idx1'] = f.index.get_level_values(0)\n",
    "f['idx2'] = f.index.get_level_values(1)\n",
    "\n",
    "print('Merging on original dataframe and computing distance.')\n",
    "f=f.merge(df, left_on='idx1', how='left', right_on='idx')\n",
    "\n",
    "f=f.merge(df, left_on='idx2', how='left', right_on='idx', suffixes=('_1','_2'))\n",
    "\n",
    "#add Haversine distance to pairs\n",
    "\n",
    "f['Distance']=np.nan\n",
    "f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']] = f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']].astype(float)\n",
    "f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull()),'Distance']=f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull())].apply(lambda row: haversine(row), axis=1)\n",
    "\n",
    "f=f[['idx1',\n",
    "     'idx2',\n",
    "     'FileName_1',\n",
    "     'FileName_2',\n",
    "     'File_Match',\n",
    "     'Name_1',\n",
    "     'Name_2',\n",
    "     'Name_DL',\n",
    "     'Name_CS',\n",
    "     'Name_Q',\n",
    "     'CleanName_DL',\n",
    "     'Type_1',\n",
    "     'Type_2',\n",
    "     'Type_Match',\n",
    "     'Address_1',\n",
    "     'Address_2',\n",
    "     'Addr_DL',\n",
    "     'Addr_CS',\n",
    "     'StrNum_Match',\n",
    "     'StrName_DL',\n",
    "     'StrName_CS',\n",
    "     'PostalCode_1',\n",
    "     'PostalCode_2',\n",
    "     'PC_Match',\n",
    "     'City_1',\n",
    "     'City_2',\n",
    "     'City_DL',\n",
    "     'CSDUID_1',\n",
    "     'CSDUID_2',\n",
    "     'Distance']]\n",
    "\n",
    "\n",
    "f.to_csv('outputs/pairs_PC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "\n",
    "#output pairs that have addresses and coordinates separately from those missing one or more addresses/coordinates\n",
    "#f.loc[(~f.Distance.isnull())&(~f.Address_1.isnull())&(~f.Address_2.isnull())].to_csv('outputs/FullInfoPC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "#f.loc[(f.Distance.isnull())|(f.Address_1.isnull())|(f.Address_2.isnull())].to_csv('outputs/PartialInfoPC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:odbiz]",
   "language": "python",
   "name": "conda-env-odbiz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
