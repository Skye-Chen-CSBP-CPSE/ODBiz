{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:40:25.259180Z",
     "iopub.status.busy": "2022-09-06T18:40:25.258459Z",
     "iopub.status.idle": "2022-09-06T18:40:52.786649Z",
     "shell.execute_reply": "2022-09-06T18:40:52.785475Z",
     "shell.execute_reply.started": "2022-09-06T18:40:25.259127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/odbiz\n",
      "\n",
      "  added / updated specs:\n",
      "    - importlib_metadata\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    importlib-metadata-4.11.4  |   py38h578d9bd_0          33 KB  http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote\n",
      "    importlib_metadata-4.11.4  |       hd8ed1ab_0           4 KB  http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote\n",
      "    zipp-3.8.1                 |     pyhd8ed1ab_0          13 KB  http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          49 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  importlib-metadata artifactory/api/conda/conda-forge-remote/linux-64::importlib-metadata-4.11.4-py38h578d9bd_0\n",
      "  importlib_metadata artifactory/api/conda/conda-forge-remote/noarch::importlib_metadata-4.11.4-hd8ed1ab_0\n",
      "  zipp               artifactory/api/conda/conda-forge-remote/noarch::zipp-3.8.1-pyhd8ed1ab_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "importlib-metadata-4 | 33 KB     | ##################################### | 100% \n",
      "zipp-3.8.1           | 13 KB     | ##################################### | 100% \n",
      "importlib_metadata-4 | 4 KB      | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install importlib_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:42:11.387535Z",
     "iopub.status.busy": "2022-09-06T18:42:11.387016Z",
     "iopub.status.idle": "2022-09-06T18:43:11.776572Z",
     "shell.execute_reply": "2022-09-06T18:43:11.775595Z",
     "shell.execute_reply.started": "2022-09-06T18:42:11.387496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - rl_helper\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote/linux-64\n",
      "  - http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install rl_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T16:35:51.427174Z",
     "iopub.status.busy": "2022-09-12T16:35:51.426194Z",
     "iopub.status.idle": "2022-09-12T16:35:51.954512Z",
     "shell.execute_reply": "2022-09-12T16:35:51.953488Z",
     "shell.execute_reply.started": "2022-09-12T16:35:51.427139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. Preprocessing - renaming columns, removing accents, and making string replacements.\n",
      "II. Record linkage - Now creating multiindex and performing comparisons\n",
      "Computing metrics for 64 candidate pairs\n",
      "processing chunk 1 of 1\n",
      "Score cut-off of 0 reduced candidate pairs to 59\n",
      "Merging on original dataframe and computing distance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#1_create_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from rl_helper import strip_accents, AddressClean, haversine\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sourcefile = '/home/jovyan/ODBiz/7-Deduplication/inputs/1NAICS_Test.json'\n",
    "with open(sourcefile) as source_f:\n",
    "    Source = json.load(source_f)\n",
    "    \n",
    "\n",
    "df = pd.read_csv(Source[\"filename\"],\n",
    "                 encoding=Source[\"encoding\"],\n",
    "                 index_col=Source[\"index\"])\n",
    "\n",
    "#df = pd.read_csv( '/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/1NAICS_Test.json', encoding=Source[\"encoding\"], index_col=Source[\"index\"])\n",
    "\n",
    "print('I. Preprocessing - renaming columns, removing accents, and making string replacements.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reduce database to only the columns we use for comparisons\n",
    "df = df[Source[\"column_map\"].values()]\n",
    "column_map = {val: key for key, val in Source[\"column_map\"].items()}\n",
    "df = df.rename(columns = column_map)\n",
    "\n",
    "# remove accents\n",
    "\n",
    "text_cols = ['Name','Address','StreetName','City']\n",
    "\n",
    "for col in text_cols:\n",
    "    df.loc[~df[col].isnull(),col]=df.loc[~df[col].isnull(),col].apply(strip_accents)\n",
    "    \n",
    "#make names lowercase\n",
    "df['Name'] = df['Name'].apply(str)\n",
    "df['Name'] = df['Name'].str.lower()\n",
    "\n",
    "df['Address'] = df['Address'].apply(str)\n",
    "df['Address'] = df['Address'].str.lower()\n",
    "\n",
    "df['StreetName'] = df['StreetName'].apply(str)\n",
    "df['StreetName'] = df['StreetName'].str.lower()\n",
    "\n",
    "#standardise addresses using \"AddressClean\" function in the rl_helper module\n",
    "\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "#remove periods, apostrophes, commas, and hypens in the Name and address columns\n",
    "\n",
    "r_list = [r\".\", r\",\", r\"'\", r\"-\"]\n",
    "\n",
    "for r in r_list:\n",
    "\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(r, ' ', regex=False)\n",
    "    df[\"Address\"] = df[\"Address\"].str.replace(r, ' ', regex=False)\n",
    "\n",
    "#remove excess whitespace\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(r\" +\", \" \", regex=True)\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(r\" +\", \" \", regex=True)\n",
    "\n",
    "#standardise postal codes - just remove empty space and make sure it's all lower case\n",
    "\n",
    "df.loc[~df.PostalCode.isnull(), 'PostalCode'] = df.loc[~df.PostalCode.isnull(), 'PostalCode'].str.replace(' ', '', regex=True).str.lower()\n",
    "\n",
    "#create an extra temporary Name column with an additional level of cleaning\n",
    "\n",
    "df['NameClean'] = clean(df[\"Name\"])\n",
    "\n",
    "#Some records have street number and street name, but no address field filled\n",
    "\n",
    "df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'Address']\\\n",
    "    =clean(df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetNumber']+' '+\\\n",
    "           df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetName']+' '+\\\n",
    "        df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'City'])\n",
    "\n",
    "#df.to_csv('NEW.csv')\n",
    "\n",
    "r\"\"\"\n",
    "II. Record Linkage\n",
    "\n",
    "\n",
    "This is the section that uses the record linkage package to determine candidate pairs,\n",
    "which will be evaluated separately.\n",
    "\"\"\"\n",
    "print('II. Record linkage - Now creating multiindex and performing comparisons')\n",
    "\n",
    "indexer = rl.Index()\n",
    "indexer.block('PostalCode')\n",
    "candidate_links = indexer.index(df)\n",
    "\n",
    "print('Computing metrics for {} candidate pairs'.format(len(candidate_links)))\n",
    "\n",
    "# likely to be a lot of records to match, so split into chunks\n",
    "n = math.ceil(len(candidate_links) / 1E5)\n",
    "chunks = rl.index_split(candidate_links, n)\n",
    "\n",
    "# Comparison step\n",
    "results = []\n",
    "\n",
    "# n_jobs specifies number of cores for running in parallel\n",
    "compare = rl.Compare(n_jobs=4)\n",
    "\n",
    "compare.exact('StreetNumber', 'StreetNumber', label='StrNum_Match')\n",
    "compare.exact('PostalCode', 'PostalCode', label='PC_Match')\n",
    "compare.exact('FileName', 'FileName', label='File_Match')\n",
    "#compare.exact('Type', 'Type', label='Type_Match')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='Addr_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='Addr_CS')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='StrName_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='StrName_CS')\n",
    "compare.string('City', 'City', method='damerau_levenshtein', label='City_DL')\n",
    "compare.string('Name', 'Name', method='damerau_levenshtein', label='Name_DL')\n",
    "compare.string('Name', 'Name', method='cosine', label='Name_CS')\n",
    "compare.string('Name', 'Name', method='qgram', label='Name_Q')\n",
    "compare.string(\"NameClean\", \"NameClean\", method='damerau_levenshtein', label=\"CleanName_DL\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    i += 1\n",
    "    print('processing chunk {} of {}'.format(i,n))\n",
    "\n",
    "    features = compare.compute(chunk, df)\n",
    "\n",
    "    #reduce comparison matrix to entries where the name score is reasonably high\n",
    "\n",
    "    cutoff = 0\n",
    "    features = features.loc[features.Name_CS > cutoff]\n",
    "    results.append(features)\n",
    "f = pd.concat(results)\n",
    "print('Score cut-off of {} reduced candidate pairs to {}'.format(cutoff, len(f)))\n",
    "\n",
    "\n",
    "\n",
    "f['idx1'] = f.index.get_level_values(0)\n",
    "f['idx2'] = f.index.get_level_values(1)\n",
    "\n",
    "print('Merging on original dataframe and computing distance.')\n",
    "f=f.merge(df, left_on='idx1', how='left', right_on='idx')\n",
    "\n",
    "f=f.merge(df, left_on='idx2', how='left', right_on='idx', suffixes=('_1','_2'))\n",
    "\n",
    "#add Haversine distance to pairs\n",
    "\n",
    "f['Distance']=np.nan\n",
    "f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']] = f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']].astype(float)\n",
    "f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull()),'Distance']=f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull())].apply(lambda row: haversine(row), axis=1)\n",
    "\n",
    "f=f[['idx1',\n",
    "     'idx2',\n",
    "     'FileName_1',\n",
    "     'FileName_2',\n",
    "     'File_Match',\n",
    "     'Name_1',\n",
    "     'Name_2',\n",
    "     'Name_DL',\n",
    "     'Name_CS',\n",
    "     'Name_Q',\n",
    "     'CleanName_DL',\n",
    "     'Address_1',\n",
    "     'Address_2',\n",
    "     'Addr_DL',\n",
    "     'Addr_CS',\n",
    "     'StrNum_Match',\n",
    "     'StrName_DL',\n",
    "     'StrName_CS',\n",
    "     'PostalCode_1',\n",
    "     'PostalCode_2',\n",
    "     'PC_Match',\n",
    "     'City_1',\n",
    "     'City_2',\n",
    "     'City_DL',\n",
    "     'CSDUID_1',\n",
    "     'CSDUID_2',\n",
    "     'Distance']]\n",
    "\n",
    "\n",
    "f.to_csv('outputs/pairs_PC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "\n",
    "#output pairs that have addresses and coordinates separately from those missing one or more addresses/coordinates\n",
    "f.loc[(~f.Distance.isnull())&(~f.Address_1.isnull())&(~f.Address_2.isnull())].to_csv('outputs/FullInfoPC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "f.loc[(f.Distance.isnull())|(f.Address_1.isnull())|(f.Address_2.isnull())].to_csv('outputs/PartialInfoPC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T16:56:32.543707Z",
     "iopub.status.busy": "2022-09-09T16:56:32.542526Z",
     "iopub.status.idle": "2022-09-09T16:56:47.515275Z",
     "shell.execute_reply": "2022-09-09T16:56:47.514181Z",
     "shell.execute_reply.started": "2022-09-09T16:56:32.543668Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. Preprocessing - renaming columns, removing accents, and making string replacements.\n",
      "II. Record linkage - Now creating multiindex and performing comparisons\n",
      "Computing metrics for 80995 candidate pairs\n",
      "processing chunk 1 of 1\n",
      "Score cut-off of 0.3 reduced candidate pairs to 5695\n",
      "Merging on original dataframe and computing distance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4_creat _pairs_csd \n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "This script looks for potential duplicates in a database\n",
    "\n",
    "We are assuming a set of standard columns,\n",
    "Name, Address, StreetNumber, StreetName, City, PostalCode, Province, Latitude, Longitude\n",
    "\n",
    "There are two sections, an initial Preprocessing step, and then the comparison script\n",
    "\n",
    "With minor modification, this can also be used for record linkage between two files.\n",
    "\n",
    "I) Preprocessing:\n",
    "    0. Read in JSON 'source' file that contains\n",
    "        i. file name of database\n",
    "        ii. mappings from database column names to standard names\n",
    "        iii. a short dictionary of terms to replace in the Name field to improve\n",
    "            potential matches (e.g., 'ch' for 'centre hospitalier')\n",
    "    1. Read in database (columns as values from json)\n",
    "    2. Strip all accents from all text fields\n",
    "    3. Process Address and Street Name fields to standardise street types\n",
    "    4. run recordlinkage's \"clean\" function to remove extra whitespace and any\n",
    "        remaining non-ascii characters, and anything in parentheses\n",
    "    5. Standardise PostalCode\n",
    "        \n",
    "II) Record Linkage:\n",
    "    Use RecordLinkageToolkit to perform comparisons and create index pairs:\n",
    "        Criteria:\n",
    "                Province - Block (consider only matches where equal)\n",
    "                Name - Damerau-Levenshtein, qgram\n",
    "                Address - Damerau-Levenshtein, Cosine\n",
    "                StreetNumber - Exact\n",
    "                StreetName - Damerau-Levenshtein, Cosine\n",
    "                City - Damerau-Levenshtein\n",
    "                PostalCode - Exact\n",
    "                Latitude/Longitude - Distance\n",
    "\n",
    "    The result is a Pandas multiindex object, which we then use to create a file\n",
    "    where every line contains the two objects being compared. \n",
    "    \n",
    "    This output file will be fed to a machine learning script for classification.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from rl_helper import strip_accents, AddressClean, haversine\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "Read in source file, data file, and rename data file columns\n",
    "'''\n",
    "sourcefile = \"/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/deduplicated_PC.json\"\n",
    "with open(sourcefile) as source_f:\n",
    "    Source = json.load(source_f)\n",
    "    \n",
    "\n",
    "df = pd.read_csv(Source[\"filename\"],\n",
    "               encoding=Source[\"encoding\"],\n",
    "               index_col=Source[\"index\"])\n",
    "\n",
    "print('I. Preprocessing - renaming columns, removing accents, and making string replacements.')\n",
    "\n",
    "# reduce database to only the columns we use for comparisons\n",
    "df = df[Source[\"column_map\"].values()]\n",
    "column_map = {val: key for key, val in Source[\"column_map\"].items()}\n",
    "df = df.rename(columns = column_map)\n",
    "\n",
    "# remove accents\n",
    "\n",
    "text_cols = ['Name','Address','StreetName','City']\n",
    "\n",
    "for col in text_cols:\n",
    "    df.loc[~df[col].isnull(),col]=df.loc[~df[col].isnull(),col].apply(strip_accents)\n",
    "    \n",
    "#make names lowercase\n",
    "df['Name'] = df['Name'].apply(str)\n",
    "df['Name'] = df['Name'].str.lower()\n",
    "\n",
    "df['Address'] = df['Address'].apply(str)\n",
    "df['Address'] = df['Address'].str.lower()\n",
    "\n",
    "df['StreetName'] = df['StreetName'].apply(str)\n",
    "df['StreetName'] = df['StreetName'].str.lower()\n",
    "\n",
    "#apply text swaps in the Name column    \n",
    "#for swap in Source[\"text_map\"]:\n",
    "    #start = r'\\b'+re.escape(swap[0])+r'\\b'\n",
    "    #df[\"Name\"] = df[\"Name\"].str.replace(start,swap[1], regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#standardise addresses using \"AddressClean\" function in the rl_helper module\n",
    "\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "#remove periods, apostrophes, commas, and hypens in the Name and address columns\n",
    "\n",
    "r_list = [r\".\", r\",\", r\"'\", r\"-\"]\n",
    "\n",
    "for r in r_list:\n",
    "\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(r, ' ', regex=False)\n",
    "    df[\"Address\"] = df[\"Address\"].str.replace(r, ' ', regex=False)\n",
    "\n",
    "#remove excess whitespace\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(r\" +\", \" \", regex=True)\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(r\" +\", \" \", regex=True)\n",
    "\n",
    "#standardise postal codes - just remove empty space and make sure it's all lower case\n",
    "\n",
    "df.loc[~df.PostalCode.isnull(), 'PostalCode'] = df.loc[~df.PostalCode.isnull(), 'PostalCode'].str.replace(' ', '', regex=True).str.lower()\n",
    "\n",
    "#create an extra temporary Name column with an additional level of cleaning\n",
    "\n",
    "df['NameClean'] = clean(df[\"Name\"])\n",
    "\n",
    "#Some records have street number and street name, but no address field filled\n",
    "\n",
    "df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'Address']\\\n",
    "    =clean(df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetNumber']+' '+\\\n",
    "           df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetName']+' '+\\\n",
    "        df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'City'])\n",
    "\n",
    "\n",
    "r\"\"\"\n",
    "II. Record Linkage\n",
    "\n",
    "\n",
    "This is the section that uses the record linkage package to determine candidate pairs,\n",
    "which will be evaluated separately.\n",
    "\"\"\"\n",
    "print('II. Record linkage - Now creating multiindex and performing comparisons')\n",
    "\n",
    "indexer = rl.Index()\n",
    "indexer.block('CSDUID')\n",
    "candidate_links = indexer.index(df)\n",
    "\n",
    "print('Computing metrics for {} candidate pairs'.format(len(candidate_links)))\n",
    "\n",
    "# likely to be a lot of records to match, so split into chunks\n",
    "n = math.ceil(len(candidate_links) / 1E5)\n",
    "chunks = rl.index_split(candidate_links, n)\n",
    "\n",
    "# Comparison step\n",
    "results = []\n",
    "\n",
    "# n_jobs specifies number of cores for running in parallel\n",
    "compare = rl.Compare(n_jobs=4)\n",
    "\n",
    "compare.exact('StreetNumber', 'StreetNumber', label='StrNum_Match')\n",
    "compare.exact('PostalCode', 'PostalCode', label='PC_Match')\n",
    "compare.exact('FileName', 'FileName', label='File_Match')\n",
    "#compare.exact('Type', 'Type', label='Type_Match')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='Addr_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='Addr_CS')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='StrName_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='StrName_CS')\n",
    "compare.string('City', 'City', method='damerau_levenshtein', label='City_DL')\n",
    "compare.string('Name', 'Name', method='damerau_levenshtein', label='Name_DL')\n",
    "compare.string('Name', 'Name', method='cosine', label='Name_CS')\n",
    "compare.string('Name', 'Name', method='qgram', label='Name_Q')\n",
    "compare.string(\"NameClean\", \"NameClean\", method='damerau_levenshtein', label=\"CleanName_DL\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    i += 1\n",
    "    print('processing chunk {} of {}'.format(i,n))\n",
    "\n",
    "    features = compare.compute(chunk, df)\n",
    "\n",
    "    #reduce comparison matrix to entries where the name score is reasonably high\n",
    "\n",
    "    cutoff = 0.3\n",
    "    features = features.loc[features.Name_CS > cutoff]\n",
    "    results.append(features)\n",
    "f = pd.concat(results)\n",
    "print('Score cut-off of {} reduced candidate pairs to {}'.format(cutoff, len(f)))\n",
    "\n",
    "f['idx1'] = f.index.get_level_values(0)\n",
    "f['idx2'] = f.index.get_level_values(1)\n",
    "\n",
    "print('Merging on original dataframe and computing distance.')\n",
    "f=f.merge(df, left_on='idx1', how='left', right_on='idx')\n",
    "\n",
    "f=f.merge(df, left_on='idx2', how='left', right_on='idx', suffixes=('_1','_2'))\n",
    "\n",
    "#add Haversine distance to pairs\n",
    "\n",
    "f['Distance']=np.nan\n",
    "f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']] = f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']].astype(float)\n",
    "f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull()),'Distance']=f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull())].apply(lambda row: haversine(row), axis=1)\n",
    "\n",
    "f=f[['idx1',\n",
    "     'idx2',\n",
    "     'FileName_1',\n",
    "     'FileName_2',\n",
    "     'File_Match',\n",
    "     'Name_1',\n",
    "     'Name_2',\n",
    "     'Name_DL',\n",
    "     'Name_CS',\n",
    "     'Name_Q',\n",
    "     'CleanName_DL',\n",
    "     'Address_1',\n",
    "     'Address_2',\n",
    "     'Addr_DL',\n",
    "     'Addr_CS',\n",
    "     'StrNum_Match',\n",
    "     'StrName_DL',\n",
    "     'StrName_CS',\n",
    "     'PostalCode_1',\n",
    "     'PostalCode_2',\n",
    "     'PC_Match',\n",
    "     'City_1',\n",
    "     'City_2',\n",
    "     'City_DL',\n",
    "     'CSDUID_1',\n",
    "     'CSDUID_2',\n",
    "     'Distance']]\n",
    "\n",
    "\n",
    "f.to_csv('outputs/pairs_CSD.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-09T20:09:02.787767Z",
     "iopub.status.busy": "2022-09-09T20:09:02.787030Z",
     "iopub.status.idle": "2022-09-09T20:09:32.920361Z",
     "shell.execute_reply": "2022-09-09T20:09:32.919284Z",
     "shell.execute_reply.started": "2022-09-09T20:09:02.787737Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. Preprocessing - renaming columns, removing accents, and making string replacements.\n",
      "II. Record linkage - Now creating multiindex and performing comparisons\n",
      "Computing metrics for 199609 candidate pairs\n",
      "processing chunk 1 of 2\n",
      "processing chunk 2 of 2\n",
      "Score cut-off of 0.5 reduced candidate pairs to 745\n",
      "Merging on original dataframe and computing distance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7_create_pairs_cihi\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "This script looks for potential duplicates in a database\n",
    "\n",
    "We are assuming a set of standard columns,\n",
    "Name, Address, StreetNumber, StreetName, City, PostalCode, Province, Latitude, Longitude\n",
    "\n",
    "There are two sections, an initial Preprocessing step, and then the comparison script\n",
    "\n",
    "With minor modification, this can also be used for record linkage between two files.\n",
    "\n",
    "I) Preprocessing:\n",
    "    0. Read in JSON 'source' file that contains\n",
    "        i. file name of database\n",
    "        ii. mappings from database column names to standard names\n",
    "        iii. a short dictionary of terms to replace in the Name field to improve\n",
    "            potential matches (e.g., 'ch' for 'centre hospitalier')\n",
    "    1. Read in database (columns as values from json)\n",
    "    2. Strip all accents from all text fields\n",
    "    3. Process Address and Street Name fields to standardise street types\n",
    "    4. run recordlinkage's \"clean\" function to remove extra whitespace and any\n",
    "        remaining non-ascii characters, and anything in parentheses\n",
    "    5. Standardise PostalCode\n",
    "        \n",
    "II) Record Linkage:\n",
    "    Use RecordLinkageToolkit to perform comparisons and create index pairs:\n",
    "        Criteria:\n",
    "                Province - Block (consider only matches where equal)\n",
    "                Name - Damerau-Levenshtein, qgram\n",
    "                Address - Damerau-Levenshtein, Cosine\n",
    "                StreetNumber - Exact\n",
    "                StreetName - Damerau-Levenshtein, Cosine\n",
    "                City - Damerau-Levenshtein\n",
    "                PostalCode - Exact\n",
    "                Latitude/Longitude - Distance\n",
    "\n",
    "    The result is a Pandas multiindex object, which we then use to create a file\n",
    "    where every line contains the two objects being compared. \n",
    "    \n",
    "    This output file will be fed to a machine learning script for classification.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from rl_helper import strip_accents, AddressClean, haversine\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "Read in source file, data file, and rename data file columns\n",
    "'''\n",
    "sourcefile = \"/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/deduplicated_CSD.json\"\n",
    "with open(sourcefile) as source_f:\n",
    "    Source = json.load(source_f)\n",
    "    \n",
    "\n",
    "df = pd.read_csv(Source[\"filename\"],\n",
    "               encoding=Source[\"encoding\"],\n",
    "               index_col=Source[\"index\"])\n",
    "\n",
    "print('I. Preprocessing - renaming columns, removing accents, and making string replacements.')\n",
    "\n",
    "# reduce database to only the columns we use for comparisons\n",
    "df = df[Source[\"column_map\"].values()]\n",
    "column_map = {val: key for key, val in Source[\"column_map\"].items()}\n",
    "df = df.rename(columns = column_map)\n",
    "\n",
    "# remove accents\n",
    "\n",
    "text_cols = ['Name','Address','StreetName','City']\n",
    "\n",
    "for col in text_cols:\n",
    "    df.loc[~df[col].isnull(),col]=df.loc[~df[col].isnull(),col].apply(strip_accents)\n",
    "    \n",
    "#make names and addresses lowercase\n",
    "df['Name'] = df['Name'].apply(str)\n",
    "df['Name'] = df['Name'].str.lower()\n",
    "\n",
    "df['Address'] = df['Address'].apply(str)\n",
    "df['Address'] = df['Address'].str.lower()\n",
    "\n",
    "df['StreetName'] = df['StreetName'].apply(str)\n",
    "df['StreetName'] = df['StreetName'].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "#standardise addresses using \"AddressClean\" function in the rl_helper module\n",
    "\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "#remove periods, apostrophes, commas, and hypens in the Name and address columns\n",
    "\n",
    "r_list = [r\".\", r\",\", r\"'\", r\"-\"]\n",
    "\n",
    "for r in r_list:\n",
    "\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(r, ' ', regex=False)\n",
    "    df[\"Address\"] = df[\"Address\"].str.replace(r, ' ', regex=False)\n",
    "\n",
    "#remove excess whitespace\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(r\" +\", \" \", regex=True)\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(r\" +\", \" \", regex=True)\n",
    "\n",
    "#standardise postal codes - just remove empty space and make sure it's all lower case\n",
    "\n",
    "df.loc[~df.PostalCode.isnull(), 'PostalCode'] = df.loc[~df.PostalCode.isnull(), 'PostalCode'].str.replace(' ', '', regex=True).str.lower()\n",
    "\n",
    "#create an extra temporary Name column with an additional level of cleaning\n",
    "\n",
    "df['NameClean'] = clean(df[\"Name\"])\n",
    "\n",
    "#Some records have street number and street name, but no address field filled\n",
    "\n",
    "df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'Address']\\\n",
    "    =clean(df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetNumber']+' '+\\\n",
    "           df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetName']+' '+\\\n",
    "        df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'City'])\n",
    "\n",
    "#df.to_csv('test.csv')\n",
    "\n",
    "r\"\"\"\n",
    "II. Record Linkage\n",
    "\n",
    "\n",
    "This is the section that uses the record linkage package to determine candidate pairs,\n",
    "which will be evaluated separately.\n",
    "\"\"\"\n",
    "print('II. Record linkage - Now creating multiindex and performing comparisons')\n",
    "\n",
    "indexer = rl.Index()\n",
    "indexer.block('Province')\n",
    "candidate_links = indexer.index(df)\n",
    "\n",
    "print('Computing metrics for {} candidate pairs'.format(len(candidate_links)))\n",
    "\n",
    "# likely to be a lot of records to match, so split into chunks\n",
    "n = math.ceil(len(candidate_links) / 1E5)\n",
    "chunks = rl.index_split(candidate_links, n)\n",
    "\n",
    "# Comparison step\n",
    "results = []\n",
    "\n",
    "# n_jobs specifies number of cores for running in parallel\n",
    "compare = rl.Compare(n_jobs=4)\n",
    "\n",
    "compare.exact('StreetNumber', 'StreetNumber', label='StrNum_Match')\n",
    "compare.exact('PostalCode', 'PostalCode', label='PC_Match')\n",
    "compare.exact('FileName', 'FileName', label='File_Match')\n",
    "#compare.exact('Type', 'Type', label='Type_Match')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='Addr_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='Addr_CS')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='StrName_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='StrName_CS')\n",
    "compare.string('City', 'City', method='damerau_levenshtein', label='City_DL')\n",
    "compare.string('Name', 'Name', method='damerau_levenshtein', label='Name_DL')\n",
    "compare.string('Name', 'Name', method='cosine', label='Name_CS')\n",
    "compare.string('Name', 'Name', method='qgram', label='Name_Q')\n",
    "compare.string(\"NameClean\", \"NameClean\", method='damerau_levenshtein', label=\"CleanName_DL\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    i += 1\n",
    "    print('processing chunk {} of {}'.format(i,n))\n",
    "\n",
    "    features = compare.compute(chunk, df)\n",
    "\n",
    "    #reduce comparison matrix to entries where the name score is reasonably high\n",
    "\n",
    "    cutoff = 0.5\n",
    "    features = features.loc[features.Name_CS > cutoff]\n",
    "    results.append(features)\n",
    "f = pd.concat(results)\n",
    "print('Score cut-off of {} reduced candidate pairs to {}'.format(cutoff, len(f)))\n",
    "\n",
    "f['idx1'] = f.index.get_level_values(0)\n",
    "f['idx2'] = f.index.get_level_values(1)\n",
    "\n",
    "print('Merging on original dataframe and computing distance.')\n",
    "f=f.merge(df, left_on='idx1', how='left', right_on='idx')\n",
    "\n",
    "f=f.merge(df, left_on='idx2', how='left', right_on='idx', suffixes=('_1','_2'))\n",
    "\n",
    "#add Haversine distance to pairs\n",
    "\n",
    "f['Distance']=np.nan\n",
    "f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']] = f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']].astype(float)\n",
    "f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull()),'Distance']=f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull())].apply(lambda row: haversine(row), axis=1)\n",
    "\n",
    "f=f[['idx1',\n",
    "     'idx2',\n",
    "     'FileName_1',\n",
    "     'FileName_2',\n",
    "     'File_Match',\n",
    "     'Name_1',\n",
    "     'Name_2',\n",
    "     'Name_DL',\n",
    "     'Name_CS',\n",
    "     'Name_Q',\n",
    "     'CleanName_DL',\n",
    "     'Address_1',\n",
    "     'Address_2',\n",
    "     'Addr_DL',\n",
    "     'Addr_CS',\n",
    "     'StrNum_Match',\n",
    "     'StrName_DL',\n",
    "     'StrName_CS',\n",
    "     'PostalCode_1',\n",
    "     'PostalCode_2',\n",
    "     'PC_Match',\n",
    "     'City_1',\n",
    "     'City_2',\n",
    "     'City_DL',\n",
    "     'CSDUID_1',\n",
    "     'CSDUID_2',\n",
    "     'Distance']]\n",
    "\n",
    "\n",
    "\n",
    "f.to_csv('outputs/pairs_CIHI.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#8_scratch_duplicates_cihi\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('outputs/pairs_CIHI.csv')\n",
    "final = pd.read_csv(\"inputs/deduplicated_CSD.csv\", low_memory=False)\n",
    "\n",
    "\n",
    "# CIHI_healthcare_facilities.csv\n",
    "\n",
    "df = df[(df['FileName_1']  == \"CIHI_healthcare_facilities.csv\") | (df['FileName_2']  == \"CIHI_healthcare_facilities.csv\" )]\n",
    "df.to_csv('outputs/pairs_CIHI.csv', index=False)\n",
    "\n",
    "\n",
    "def hierarchy(id1, id2): \n",
    "\n",
    "#    try:\n",
    "\n",
    "    check_file1 = final.loc[final.idx == id1].filename.item()\n",
    "    check_file2 = final.loc[final.idx == id2].filename.item()\n",
    "    check_name1 = final.loc[final.idx == id1].facility_name.item()\n",
    "    check_name2 = final.loc[final.idx == id2].facility_name.item()\n",
    "\n",
    "\n",
    "    if check_file1 == 'CIHI_healthcare_facilities.csv' and check_file2 != 'CIHI_healthcare_facilities.csv':\n",
    "        return id1\n",
    "\n",
    "    elif check_file2 == 'CIHI_healthcare_facilities.csv' and check_file1 != 'CIHI_healthcare_facilities.csv':\n",
    "        return id2\n",
    "\n",
    "    else:\n",
    "        return id1\n",
    "\n",
    "#    except ValueError:\n",
    " #       return \"Item not found.\"\n",
    "\n",
    "df[\"to_remove\"] = df.apply(lambda x: hierarchy( id1 = x.idx1, id2 = x.idx2), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_1(name, distance, postalcode):\n",
    "\n",
    "    if distance != None:\n",
    "        if name > 0.5 and distance < 1 and postalcode == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df[\"Check_1\"] = df.apply(lambda x: check_1(name = x.Name_CS, distance = x.Distance, postalcode = x.PC_Match), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_2(name, distance, postalcode):\n",
    "\n",
    "    if distance != None:\n",
    "        if name > 0.85 and distance < 5 and postalcode == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df[\"Check_2\"] = df.apply(lambda x: check_2(name = x.Name_CS, distance = x.Distance, postalcode = x.PC_Match), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_3(name, distance):\n",
    "\n",
    "    if distance != None:\n",
    "        if name > 0.95 and distance < 5:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df[\"Check_3\"] = df.apply(lambda x: check_3(name = x.Name_CS, distance = x.Distance), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_4(name):\n",
    "\n",
    "    if name > 0.99:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df[\"Check_4\"] = df.apply(lambda x: check_4(name = x.Name_CS), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rows = df.index[(df['Check_1'] == True) | (df['Check_2'] == True) | (df['Check_3'] == True) | (df['Check_4'] == True)].tolist()\n",
    "false_rows = df.index[(df['Check_1'] == False) & (df['Check_2'] == False) & (df['Check_3'] == False) | (df['Check_4'] == True)].tolist()\n",
    "\n",
    "df_true = df.loc[rows]\n",
    "df_false = df.loc[false_rows]\n",
    "\n",
    "df_true.to_csv('outputs/duplicates_CIHI.csv', index=False)\n",
    "df_false.to_csv('outputs/NOT_duplicates_CIHI.csv', index=False)\n",
    "\n",
    "hashes = df_true['to_remove'].tolist()\n",
    "\n",
    "final = final[~final['idx'].isin(hashes)]\n",
    "final.to_csv('deduplicated_CIHI.csv', index=False)\n",
    "final.to_csv('inputs/deduplicated_CIHI.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9_creat_pairs_Prov\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from rl_helper import strip_accents, AddressClean, haversine\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "Read in source file, data file, and rename data file columns\n",
    "'''\n",
    "sourcefile = \"/home/jovyan/data-vol-1/ODHF/LODE-ECDO/scripts/HealthFacilities/V2/7-Deduplication/inputs/deduplicated_CIHI.json\"\n",
    "with open(sourcefile) as source_f:\n",
    "    Source = json.load(source_f)\n",
    "    \n",
    "\n",
    "df = pd.read_csv(Source[\"filename\"],\n",
    "               encoding=Source[\"encoding\"],\n",
    "               index_col=Source[\"index\"])\n",
    "\n",
    "print('I. Preprocessing - renaming columns, removing accents, and making string replacements.')\n",
    "\n",
    "# reduce database to only the columns we use for comparisons\n",
    "df = df[Source[\"column_map\"].values()]\n",
    "column_map = {val: key for key, val in Source[\"column_map\"].items()}\n",
    "df = df.rename(columns = column_map)\n",
    "\n",
    "# remove accents\n",
    "\n",
    "text_cols = ['Name','Address','StreetName','City']\n",
    "\n",
    "for col in text_cols:\n",
    "    df.loc[~df[col].isnull(),col]=df.loc[~df[col].isnull(),col].apply(strip_accents)\n",
    "    \n",
    "#make names and addresses lowercase\n",
    "df['Name'] = df['Name'].apply(str)\n",
    "df['Name'] = df['Name'].str.lower()\n",
    "\n",
    "df['Address'] = df['Address'].apply(str)\n",
    "df['Address'] = df['Address'].str.lower()\n",
    "\n",
    "df['StreetName'] = df['StreetName'].apply(str)\n",
    "df['StreetName'] = df['StreetName'].str.lower()\n",
    "\n",
    "#apply text swaps in the Name column    \n",
    "for swap in Source[\"text_map\"]:\n",
    "    start = r'\\b'+re.escape(swap[0])+r'\\b'\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(start,swap[1], regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#standardise addresses using \"AddressClean\" function in the rl_helper module\n",
    "\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "#remove periods, apostrophes, commas, and hypens in the Name and address columns\n",
    "\n",
    "r_list = [r\".\", r\",\", r\"'\", r\"-\"]\n",
    "\n",
    "for r in r_list:\n",
    "\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(r, ' ', regex=False)\n",
    "    df[\"Address\"] = df[\"Address\"].str.replace(r, ' ', regex=False)\n",
    "\n",
    "#remove excess whitespace\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(r\" +\", \" \", regex=True)\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(r\" +\", \" \", regex=True)\n",
    "\n",
    "#standardise postal codes - just remove empty space and make sure it's all lower case\n",
    "\n",
    "df.loc[~df.PostalCode.isnull(), 'PostalCode'] = df.loc[~df.PostalCode.isnull(), 'PostalCode'].str.replace(' ', '', regex=True).str.lower()\n",
    "\n",
    "#create an extra temporary Name column with an additional level of cleaning\n",
    "\n",
    "df['NameClean'] = clean(df[\"Name\"])\n",
    "\n",
    "#Some records have street number and street name, but no address field filled\n",
    "\n",
    "df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'Address']\\\n",
    "    =clean(df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetNumber']+' '+\\\n",
    "           df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetName']+' '+\\\n",
    "        df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'City'])\n",
    "\n",
    "#df.to_csv('test.csv')\n",
    "\n",
    "r\"\"\"\n",
    "II. Record Linkage\n",
    "\n",
    "\n",
    "This is the section that uses the record linkage package to determine candidate pairs,\n",
    "which will be evaluated separately.\n",
    "\"\"\"\n",
    "print('II. Record linkage - Now creating multiindex and performing comparisons')\n",
    "\n",
    "indexer = rl.Index()\n",
    "indexer.block('Province')\n",
    "candidate_links = indexer.index(df)\n",
    "\n",
    "print('Computing metrics for {} candidate pairs'.format(len(candidate_links)))\n",
    "\n",
    "# likely to be a lot of records to match, so split into chunks\n",
    "n = math.ceil(len(candidate_links) / 1E5)\n",
    "chunks = rl.index_split(candidate_links, n)\n",
    "\n",
    "# Comparison step\n",
    "results = []\n",
    "\n",
    "# n_jobs specifies number of cores for running in parallel\n",
    "compare = rl.Compare(n_jobs=4)\n",
    "\n",
    "compare.exact('StreetNumber', 'StreetNumber', label='StrNum_Match')\n",
    "compare.exact('PostalCode', 'PostalCode', label='PC_Match')\n",
    "compare.exact('FileName', 'FileName', label='File_Match')\n",
    "compare.exact('Type', 'Type', label='Type_Match')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='Addr_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='Addr_CS')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='StrName_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='StrName_CS')\n",
    "compare.string('City', 'City', method='damerau_levenshtein', label='City_DL')\n",
    "compare.string('Name', 'Name', method='damerau_levenshtein', label='Name_DL')\n",
    "compare.string('Name', 'Name', method='cosine', label='Name_CS')\n",
    "compare.string('Name', 'Name', method='qgram', label='Name_Q')\n",
    "compare.string(\"NameClean\", \"NameClean\", method='damerau_levenshtein', label=\"CleanName_DL\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    i += 1\n",
    "    print('processing chunk {} of {}'.format(i,n))\n",
    "\n",
    "    features = compare.compute(chunk, df)\n",
    "\n",
    "    #reduce comparison matrix to entries where the name score is reasonably high\n",
    "\n",
    "    cutoff = 0.5\n",
    "    features = features.loc[features.Name_CS > cutoff]\n",
    "    results.append(features)\n",
    "f = pd.concat(results)\n",
    "print('Score cut-off of {} reduced candidate pairs to {}'.format(cutoff, len(f)))\n",
    "\n",
    "f['idx1'] = f.index.get_level_values(0)\n",
    "f['idx2'] = f.index.get_level_values(1)\n",
    "\n",
    "print('Merging on original dataframe and computing distance.')\n",
    "f=f.merge(df, left_on='idx1', how='left', right_on='idx')\n",
    "\n",
    "f=f.merge(df, left_on='idx2', how='left', right_on='idx', suffixes=('_1','_2'))\n",
    "\n",
    "#add Haversine distance to pairs\n",
    "\n",
    "f['Distance']=np.nan\n",
    "f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']] = f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']].astype(float)\n",
    "f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull()),'Distance']=f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull())].apply(lambda row: haversine(row), axis=1)\n",
    "\n",
    "f=f[['idx1',\n",
    "     'idx2',\n",
    "     'FileName_1',\n",
    "     'FileName_2',\n",
    "     'File_Match',\n",
    "     'Name_1',\n",
    "     'Name_2',\n",
    "     'Name_DL',\n",
    "     'Name_CS',\n",
    "     'Name_Q',\n",
    "     'CleanName_DL',\n",
    "     'Type_1',\n",
    "     'Type_2',\n",
    "     'Type_Match',\n",
    "     'Address_1',\n",
    "     'Address_2',\n",
    "     'Addr_DL',\n",
    "     'Addr_CS',\n",
    "     'StrNum_Match',\n",
    "     'StrName_DL',\n",
    "     'StrName_CS',\n",
    "     'PostalCode_1',\n",
    "     'PostalCode_2',\n",
    "     'PC_Match',\n",
    "     'City_1',\n",
    "     'City_2',\n",
    "     'City_DL',\n",
    "     'CSDUID_1',\n",
    "     'CSDUID_2',\n",
    "     'Distance']]\n",
    "\n",
    "\n",
    "\n",
    "f.to_csv('outputs/pairs_PROV.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:odbiz]",
   "language": "python",
   "name": "conda-env-odbiz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
