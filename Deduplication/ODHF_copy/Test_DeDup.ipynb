{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:41:11.324511Z",
     "iopub.status.busy": "2022-09-06T18:41:11.323834Z",
     "iopub.status.idle": "2022-09-06T18:41:13.491332Z",
     "shell.execute_reply": "2022-09-06T18:41:13.490070Z",
     "shell.execute_reply.started": "2022-09-06T18:41:11.324406Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/attrdict/mapping.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/attrdict/mixins.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/astroid/node_classes.py:94: DeprecationWarning: The 'astroid.node_classes' module is deprecated and will be replaced by 'astroid.nodes' in astroid 3.0.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'strip_accents' from 'rl_helper' (/home/jovyan/.local/lib/python3.8/site-packages/rl_helper/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m strip_accents, AddressClean, haversine\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrecordlinkage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrecordlinkage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'strip_accents' from 'rl_helper' (/home/jovyan/.local/lib/python3.8/site-packages/rl_helper/__init__.py)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "This script looks for potential duplicates in a database\n",
    "\n",
    "We are assuming a set of standard columns,\n",
    "Name, Address, StreetNumber, StreetName, City, PostalCode, Province, Latitude, Longitude\n",
    "\n",
    "There are two sections, an initial Preprocessing step, and then the comparison script\n",
    "\n",
    "With minor modification, this can also be used for record linkage between two files.\n",
    "\n",
    "I) Preprocessing:\n",
    "    0. Read in JSON 'source' file that contains\n",
    "        i. file name of database\n",
    "        ii. mappings from database column names to standard names\n",
    "        iii. a short dictionary of terms to replace in the Name field to improve\n",
    "            potential matches (e.g., 'ch' for 'centre hospitalier')\n",
    "    1. Read in database (columns as values from json)\n",
    "    2. Strip all accents from all text fields\n",
    "    3. Process Address and Street Name fields to standardise street types\n",
    "    4. run recordlinkage's \"clean\" function to remove extra whitespace and any\n",
    "        remaining non-ascii characters, and anything in parentheses\n",
    "    5. Standardise PostalCode\n",
    "        \n",
    "II) Record Linkage:\n",
    "    Use RecordLinkageToolkit to perform comparisons and create index pairs:\n",
    "        Criteria:\n",
    "                Province - Block (consider only matches where equal)\n",
    "                Name - Damerau-Levenshtein, qgram\n",
    "                Address - Damerau-Levenshtein, Cosine\n",
    "                StreetNumber - Exact\n",
    "                StreetName - Damerau-Levenshtein, Cosine\n",
    "                City - Damerau-Levenshtein\n",
    "                PostalCode - Exact\n",
    "                Latitude/Longitude - Distance\n",
    "\n",
    "    The result is a Pandas multiindex object, which we then use to create a file\n",
    "    where every line contains the two objects being compared. \n",
    "    \n",
    "    This output file will be fed to a machine learning script for classification.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from rl_helper import strip_accents, AddressClean, haversine\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "'''\n",
    "Read in source file, data file, and rename data file columns\n",
    "'''\n",
    "sourcefile = '/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/1NAICS_Test.json'\n",
    "with open(sourcefile) as source_f:\n",
    "    Source = json.load(source_f)\n",
    "    \n",
    "\n",
    "df = pd.read_csv(Source[\"filename\"],\n",
    "                 encoding=Source[\"encoding\"],\n",
    "                 index_col=Source[\"index\"])\n",
    "\n",
    "#df = pd.read_csv( '/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/1NAICS_Test.json', encoding=Source[\"encoding\"], index_col=Source[\"index\"])\n",
    "\n",
    "print('I. Preprocessing - renaming columns, removing accents, and making string replacements.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reduce database to only the columns we use for comparisons\n",
    "df = df[Source[\"column_map\"].values()]\n",
    "column_map = {val: key for key, val in Source[\"column_map\"].items()}\n",
    "df = df.rename(columns = column_map)\n",
    "\n",
    "# remove accents\n",
    "\n",
    "text_cols = ['Name','Address','StreetName','City']\n",
    "\n",
    "for col in text_cols:\n",
    "    df.loc[~df[col].isnull(),col]=df.loc[~df[col].isnull(),col].apply(strip_accents)\n",
    "    \n",
    "#make names lowercase\n",
    "df['Name'] = df['Name'].apply(str)\n",
    "df['Name'] = df['Name'].str.lower()\n",
    "\n",
    "df['Address'] = df['Address'].apply(str)\n",
    "df['Address'] = df['Address'].str.lower()\n",
    "\n",
    "df['StreetName'] = df['StreetName'].apply(str)\n",
    "df['StreetName'] = df['StreetName'].str.lower()\n",
    "\n",
    "\n",
    "''' \n",
    "#apply text swaps in the Name column    \n",
    "for swap in Source[\"text_map\"]:\n",
    "    start = r'\\b'+re.escape(swap[0])+r'\\b'\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(start,swap[1], regex=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#standardise addresses using \"AddressClean\" function in the rl_helper module\n",
    "\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "#remove periods, apostrophes, commas, and hypens in the Name and address columns\n",
    "\n",
    "r_list = [r\".\", r\",\", r\"'\", r\"-\"]\n",
    "\n",
    "for r in r_list:\n",
    "\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(r, ' ', regex=False)\n",
    "    df[\"Address\"] = df[\"Address\"].str.replace(r, ' ', regex=False)\n",
    "\n",
    "#remove excess whitespace\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(r\" +\", \" \", regex=True)\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(r\" +\", \" \", regex=True)\n",
    "\n",
    "#standardise postal codes - just remove empty space and make sure it's all lower case\n",
    "\n",
    "df.loc[~df.PostalCode.isnull(), 'PostalCode'] = df.loc[~df.PostalCode.isnull(), 'PostalCode'].str.replace(' ', '', regex=True).str.lower()\n",
    "\n",
    "#create an extra temporary Name column with an additional level of cleaning\n",
    "\n",
    "df['NameClean'] = clean(df[\"Name\"])\n",
    "\n",
    "#Some records have street number and street name, but no address field filled\n",
    "\n",
    "df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'Address']\\\n",
    "    =clean(df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetNumber']+' '+\\\n",
    "           df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetName']+' '+\\\n",
    "        df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'City'])\n",
    "\n",
    "#df.to_csv('test.csv')\n",
    "\n",
    "''' \n",
    "\n",
    "\n",
    "r\"\"\"\n",
    "II. Record Linkage\n",
    "\n",
    "\n",
    "This is the section that uses the record linkage package to determine candidate pairs,\n",
    "which will be evaluated separately.\n",
    "\"\"\"\n",
    "print('II. Record linkage - Now creating multiindex and performing comparisons')\n",
    "\n",
    "indexer = rl.Index()\n",
    "indexer.block('PostalCode')\n",
    "candidate_links = indexer.index(df)\n",
    "\n",
    "print('Computing metrics for {} candidate pairs'.format(len(candidate_links)))\n",
    "\n",
    "# likely to be a lot of records to match, so split into chunks\n",
    "n = math.ceil(len(candidate_links) / 1E5)\n",
    "chunks = rl.index_split(candidate_links, n)\n",
    "\n",
    "# Comparison step\n",
    "results = []\n",
    "\n",
    "# n_jobs specifies number of cores for running in parallel\n",
    "compare = rl.Compare(n_jobs=4)\n",
    "\n",
    "compare.exact('StreetNumber', 'StreetNumber', label='StrNum_Match')\n",
    "compare.exact('PostalCode', 'PostalCode', label='PC_Match')\n",
    "compare.exact('FileName', 'FileName', label='File_Match')\n",
    "compare.exact('Type', 'Type', label='Type_Match')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='Addr_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='Addr_CS')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='StrName_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='StrName_CS')\n",
    "compare.string('City', 'City', method='damerau_levenshtein', label='City_DL')\n",
    "compare.string('Name', 'Name', method='damerau_levenshtein', label='Name_DL')\n",
    "compare.string('Name', 'Name', method='cosine', label='Name_CS')\n",
    "compare.string('Name', 'Name', method='qgram', label='Name_Q')\n",
    "compare.string(\"NameClean\", \"NameClean\", method='damerau_levenshtein', label=\"CleanName_DL\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    i += 1\n",
    "    print('processing chunk {} of {}'.format(i,n))\n",
    "\n",
    "    features = compare.compute(chunk, df)\n",
    "\n",
    "    #reduce comparison matrix to entries where the name score is reasonably high\n",
    "\n",
    "    cutoff = 0\n",
    "    features = features.loc[features.Name_CS > cutoff]\n",
    "    results.append(features)\n",
    "f = pd.concat(results)\n",
    "print('Score cut-off of {} reduced candidate pairs to {}'.format(cutoff, len(f)))\n",
    "\n",
    "f['idx1'] = f.index.get_level_values(0)\n",
    "f['idx2'] = f.index.get_level_values(1)\n",
    "\n",
    "print('Merging on original dataframe and computing distance.')\n",
    "f=f.merge(df, left_on='idx1', how='left', right_on='idx')\n",
    "\n",
    "f=f.merge(df, left_on='idx2', how='left', right_on='idx', suffixes=('_1','_2'))\n",
    "\n",
    "#add Haversine distance to pairs\n",
    "\n",
    "f['Distance']=np.nan\n",
    "f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']] = f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']].astype(float)\n",
    "f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull()),'Distance']=f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull())].apply(lambda row: haversine(row), axis=1)\n",
    "\n",
    "f=f[['idx1',\n",
    "     'idx2',\n",
    "     'FileName_1',\n",
    "     'FileName_2',\n",
    "     'File_Match',\n",
    "     'Name_1',\n",
    "     'Name_2',\n",
    "     'Name_DL',\n",
    "     'Name_CS',\n",
    "     'Name_Q',\n",
    "     'CleanName_DL',\n",
    "     'Type_1',\n",
    "     'Type_2',\n",
    "     'Type_Match',\n",
    "     'Address_1',\n",
    "     'Address_2',\n",
    "     'Addr_DL',\n",
    "     'Addr_CS',\n",
    "     'StrNum_Match',\n",
    "     'StrName_DL',\n",
    "     'StrName_CS',\n",
    "     'PostalCode_1',\n",
    "     'PostalCode_2',\n",
    "     'PC_Match',\n",
    "     'City_1',\n",
    "     'City_2',\n",
    "     'City_DL',\n",
    "     'CSDUID_1',\n",
    "     'CSDUID_2',\n",
    "     'Distance']]\n",
    "\n",
    "\n",
    "f.to_csv('outputs/pairs_PC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "\n",
    "#output pairs that have addresses and coordinates separately from those missing one or more addresses/coordinates\n",
    "#f.loc[(~f.Distance.isnull())&(~f.Address_1.isnull())&(~f.Address_2.isnull())].to_csv('outputs/FullInfoPC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "#f.loc[(f.Distance.isnull())|(f.Address_1.isnull())|(f.Address_2.isnull())].to_csv('outputs/PartialInfoPC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:40:25.259180Z",
     "iopub.status.busy": "2022-09-06T18:40:25.258459Z",
     "iopub.status.idle": "2022-09-06T18:40:52.786649Z",
     "shell.execute_reply": "2022-09-06T18:40:52.785475Z",
     "shell.execute_reply.started": "2022-09-06T18:40:25.259127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/odbiz\n",
      "\n",
      "  added / updated specs:\n",
      "    - importlib_metadata\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    importlib-metadata-4.11.4  |   py38h578d9bd_0          33 KB  http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote\n",
      "    importlib_metadata-4.11.4  |       hd8ed1ab_0           4 KB  http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote\n",
      "    zipp-3.8.1                 |     pyhd8ed1ab_0          13 KB  http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          49 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  importlib-metadata artifactory/api/conda/conda-forge-remote/linux-64::importlib-metadata-4.11.4-py38h578d9bd_0\n",
      "  importlib_metadata artifactory/api/conda/conda-forge-remote/noarch::importlib_metadata-4.11.4-hd8ed1ab_0\n",
      "  zipp               artifactory/api/conda/conda-forge-remote/noarch::zipp-3.8.1-pyhd8ed1ab_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "importlib-metadata-4 | 33 KB     | ##################################### | 100% \n",
      "zipp-3.8.1           | 13 KB     | ##################################### | 100% \n",
      "importlib_metadata-4 | 4 KB      | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install importlib_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T18:42:11.387535Z",
     "iopub.status.busy": "2022-09-06T18:42:11.387016Z",
     "iopub.status.idle": "2022-09-06T18:43:11.776572Z",
     "shell.execute_reply": "2022-09-06T18:43:11.775595Z",
     "shell.execute_reply.started": "2022-09-06T18:42:11.387496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - rl_helper\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote/linux-64\n",
      "  - http://jfrog-platform-artifactory-ha.jfrog-system:8081/artifactory/api/conda/conda-forge-remote/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install rl_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-06T22:12:12.380526Z",
     "iopub.status.busy": "2022-09-06T22:12:12.379720Z",
     "iopub.status.idle": "2022-09-06T22:12:12.937236Z",
     "shell.execute_reply": "2022-09-06T22:12:12.936319Z",
     "shell.execute_reply.started": "2022-09-06T22:12:12.380493Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. Preprocessing - renaming columns, removing accents, and making string replacements.\n",
      "II. Record linkage - Now creating multiindex and performing comparisons\n",
      "Computing metrics for 64 candidate pairs\n",
      "processing chunk 1 of 1\n",
      "Score cut-off of 0 reduced candidate pairs to 59\n",
      "Merging on original dataframe and computing distance.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from rl_helper import strip_accents, AddressClean, haversine\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sourcefile = '/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/1NAICS_Test.json'\n",
    "with open(sourcefile) as source_f:\n",
    "    Source = json.load(source_f)\n",
    "    \n",
    "\n",
    "df = pd.read_csv(Source[\"filename\"],\n",
    "                 encoding=Source[\"encoding\"],\n",
    "                 index_col=Source[\"index\"])\n",
    "\n",
    "#df = pd.read_csv( '/home/jovyan/ODBiz/Deduplication/ODHF_copy/inputs/1NAICS_Test.json', encoding=Source[\"encoding\"], index_col=Source[\"index\"])\n",
    "\n",
    "print('I. Preprocessing - renaming columns, removing accents, and making string replacements.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reduce database to only the columns we use for comparisons\n",
    "df = df[Source[\"column_map\"].values()]\n",
    "column_map = {val: key for key, val in Source[\"column_map\"].items()}\n",
    "df = df.rename(columns = column_map)\n",
    "\n",
    "# remove accents\n",
    "\n",
    "text_cols = ['Name','Address','StreetName','City']\n",
    "\n",
    "for col in text_cols:\n",
    "    df.loc[~df[col].isnull(),col]=df.loc[~df[col].isnull(),col].apply(strip_accents)\n",
    "    \n",
    "#make names lowercase\n",
    "df['Name'] = df['Name'].apply(str)\n",
    "df['Name'] = df['Name'].str.lower()\n",
    "\n",
    "df['Address'] = df['Address'].apply(str)\n",
    "df['Address'] = df['Address'].str.lower()\n",
    "\n",
    "df['StreetName'] = df['StreetName'].apply(str)\n",
    "df['StreetName'] = df['StreetName'].str.lower()\n",
    "\n",
    "#standardise addresses using \"AddressClean\" function in the rl_helper module\n",
    "\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province != 'qc'), 'StreetName'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'] = df.loc[(~df['StreetName'].isnull()) & (df.Province == 'qc'), 'StreetName'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province != 'qc'), 'Address'].apply(AddressClean, args = ('en',))\n",
    "df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'] = df.loc[(~df['Address'].isnull()) & (df.Province == 'qc'), 'Address'].apply(AddressClean, args = ('fr',))\n",
    "\n",
    "#remove periods, apostrophes, commas, and hypens in the Name and address columns\n",
    "\n",
    "r_list = [r\".\", r\",\", r\"'\", r\"-\"]\n",
    "\n",
    "for r in r_list:\n",
    "\n",
    "    df[\"Name\"] = df[\"Name\"].str.replace(r, ' ', regex=False)\n",
    "    df[\"Address\"] = df[\"Address\"].str.replace(r, ' ', regex=False)\n",
    "\n",
    "#remove excess whitespace\n",
    "df[\"Name\"] = df[\"Name\"].str.replace(r\" +\", \" \", regex=True)\n",
    "df[\"Address\"] = df[\"Address\"].str.replace(r\" +\", \" \", regex=True)\n",
    "\n",
    "#standardise postal codes - just remove empty space and make sure it's all lower case\n",
    "\n",
    "df.loc[~df.PostalCode.isnull(), 'PostalCode'] = df.loc[~df.PostalCode.isnull(), 'PostalCode'].str.replace(' ', '', regex=True).str.lower()\n",
    "\n",
    "#create an extra temporary Name column with an additional level of cleaning\n",
    "\n",
    "df['NameClean'] = clean(df[\"Name\"])\n",
    "\n",
    "#Some records have street number and street name, but no address field filled\n",
    "\n",
    "df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'Address']\\\n",
    "    =clean(df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetNumber']+' '+\\\n",
    "           df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'StreetName']+' '+\\\n",
    "        df.loc[(df.Address.isnull())&\\\n",
    "       (~df.StreetName.isnull()),'City'])\n",
    "\n",
    "df.to_csv('NEW.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "r\"\"\"\n",
    "II. Record Linkage\n",
    "\n",
    "\n",
    "This is the section that uses the record linkage package to determine candidate pairs,\n",
    "which will be evaluated separately.\n",
    "\"\"\"\n",
    "print('II. Record linkage - Now creating multiindex and performing comparisons')\n",
    "\n",
    "indexer = rl.Index()\n",
    "indexer.block('PostalCode')\n",
    "candidate_links = indexer.index(df)\n",
    "\n",
    "print('Computing metrics for {} candidate pairs'.format(len(candidate_links)))\n",
    "\n",
    "# likely to be a lot of records to match, so split into chunks\n",
    "n = math.ceil(len(candidate_links) / 1E5)\n",
    "chunks = rl.index_split(candidate_links, n)\n",
    "\n",
    "# Comparison step\n",
    "results = []\n",
    "\n",
    "# n_jobs specifies number of cores for running in parallel\n",
    "compare = rl.Compare(n_jobs=4)\n",
    "\n",
    "compare.exact('StreetNumber', 'StreetNumber', label='StrNum_Match')\n",
    "compare.exact('PostalCode', 'PostalCode', label='PC_Match')\n",
    "compare.exact('FileName', 'FileName', label='File_Match')\n",
    "#compare.exact('Type', 'Type', label='Type_Match')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='Addr_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='Addr_CS')\n",
    "compare.string('Address', 'Address', method='damerau_levenshtein', label='StrName_DL')\n",
    "compare.string('Address', 'Address', method='cosine', label='StrName_CS')\n",
    "compare.string('City', 'City', method='damerau_levenshtein', label='City_DL')\n",
    "compare.string('Name', 'Name', method='damerau_levenshtein', label='Name_DL')\n",
    "compare.string('Name', 'Name', method='cosine', label='Name_CS')\n",
    "compare.string('Name', 'Name', method='qgram', label='Name_Q')\n",
    "compare.string(\"NameClean\", \"NameClean\", method='damerau_levenshtein', label=\"CleanName_DL\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    i += 1\n",
    "    print('processing chunk {} of {}'.format(i,n))\n",
    "\n",
    "    features = compare.compute(chunk, df)\n",
    "\n",
    "    #reduce comparison matrix to entries where the name score is reasonably high\n",
    "\n",
    "    cutoff = 0\n",
    "    features = features.loc[features.Name_CS > cutoff]\n",
    "    results.append(features)\n",
    "f = pd.concat(results)\n",
    "print('Score cut-off of {} reduced candidate pairs to {}'.format(cutoff, len(f)))\n",
    "\n",
    "\n",
    "\n",
    "f['idx1'] = f.index.get_level_values(0)\n",
    "f['idx2'] = f.index.get_level_values(1)\n",
    "\n",
    "print('Merging on original dataframe and computing distance.')\n",
    "f=f.merge(df, left_on='idx1', how='left', right_on='idx')\n",
    "\n",
    "f=f.merge(df, left_on='idx2', how='left', right_on='idx', suffixes=('_1','_2'))\n",
    "\n",
    "#add Haversine distance to pairs\n",
    "\n",
    "f['Distance']=np.nan\n",
    "f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']] = f[['Latitude_1', 'Latitude_2', 'Longitude_1', 'Longitude_2']].astype(float)\n",
    "f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull()),'Distance']=f.loc[(~f.Latitude_1.isnull())&(~f.Latitude_2.isnull())].apply(lambda row: haversine(row), axis=1)\n",
    "\n",
    "f=f[['idx1',\n",
    "     'idx2',\n",
    "     'FileName_1',\n",
    "     'FileName_2',\n",
    "     'File_Match',\n",
    "     'Name_1',\n",
    "     'Name_2',\n",
    "     'Name_DL',\n",
    "     'Name_CS',\n",
    "     'Name_Q',\n",
    "     'CleanName_DL',\n",
    "     'Address_1',\n",
    "     'Address_2',\n",
    "     'Addr_DL',\n",
    "     'Addr_CS',\n",
    "     'StrNum_Match',\n",
    "     'StrName_DL',\n",
    "     'StrName_CS',\n",
    "     'PostalCode_1',\n",
    "     'PostalCode_2',\n",
    "     'PC_Match',\n",
    "     'City_1',\n",
    "     'City_2',\n",
    "     'City_DL',\n",
    "     'CSDUID_1',\n",
    "     'CSDUID_2',\n",
    "     'Distance']]\n",
    "\n",
    "\n",
    "f.to_csv('outputs/pairs_PC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "\n",
    "#output pairs that have addresses and coordinates separately from those missing one or more addresses/coordinates\n",
    "f.loc[(~f.Distance.isnull())&(~f.Address_1.isnull())&(~f.Address_2.isnull())].to_csv('outputs/FullInfoPC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n",
    "f.loc[(f.Distance.isnull())|(f.Address_1.isnull())|(f.Address_2.isnull())].to_csv('outputs/PartialInfoPC.csv'.format(Source[\"output_name\"]),index=False,encoding='cp1252')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:odbiz]",
   "language": "python",
   "name": "conda-env-odbiz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
