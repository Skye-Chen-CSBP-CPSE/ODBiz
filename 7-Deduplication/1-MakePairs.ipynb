{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:12:13.353276Z",
     "iopub.status.busy": "2023-02-16T04:12:13.352996Z",
     "iopub.status.idle": "2023-02-16T04:12:13.356757Z",
     "shell.execute_reply": "2023-02-16T04:12:13.356337Z",
     "shell.execute_reply.started": "2023-02-16T04:12:13.353257Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This script identifies pairs of potential duplicates \n",
    "# And filters on business name and business status\n",
    "\n",
    "# We use pandas deduplicate functions to remove exact matches\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "# import re\n",
    "from rl_helper import strip_accents, AddressClean, haversine\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean\n",
    "# from recordlinkage.index import Block\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import mitosheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:12:13.973595Z",
     "iopub.status.busy": "2023-02-16T04:12:13.973390Z",
     "iopub.status.idle": "2023-02-16T04:12:18.174797Z",
     "shell.execute_reply": "2023-02-16T04:12:18.174104Z",
     "shell.execute_reply.started": "2023-02-16T04:12:13.973580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORT DATA\n",
    "\n",
    "# Read in deduplicate parameters from source json file\n",
    "# sourcefile=r\"ODBiz_Source.json\"\n",
    "# with open(sourcefile) as source_f:\n",
    "#     Source=json.load(source_f)\n",
    "\n",
    "# Read in database\n",
    "df = pd.read_csv(\"~/ODBiz/6-AssignCSDs/NAICS_Final.csv\", low_memory=False, dtype='str')\n",
    "\n",
    "# TODO we can skip the next step\n",
    "\n",
    "# Reduce database to only the columns we use for comparisons\n",
    "# DF=DF[Source[\"column_map\"].values()]\n",
    "# column_map={val: key for key, val in Source[\"column_map\"].items()}\n",
    "# DF=DF.rename(columns=column_map)\n",
    "# df=DF.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:12:18.176026Z",
     "iopub.status.busy": "2023-02-16T04:12:18.175860Z",
     "iopub.status.idle": "2023-02-16T04:12:22.235906Z",
     "shell.execute_reply": "2023-02-16T04:12:22.235238Z",
     "shell.execute_reply.started": "2023-02-16T04:12:18.176010Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FORMATTTING\n",
    "\n",
    "# Note/ todo\n",
    "# i dont want to reformat existing columns. i want to make new ones and delete them at the end\n",
    "\n",
    "# Remove accents\n",
    "text_cols=['business_name','full_address','formatted_en','city']\n",
    "for col in text_cols:\n",
    "    df.loc[~df[col].isnull(),col]=df.loc[~df[col].isnull(),col].apply(strip_accents)\n",
    "\n",
    "# Remove periods, apostrophes, commas, and hypens in the Name and address columns\n",
    "r_list=[r\".\",r\",\",r\"'\",r\"-\"]\n",
    "\n",
    "for r in r_list:\n",
    "    df[\"business_name\"] = df[\"business_name\"].str.replace(r,' ',regex=False)\n",
    "    df['full_address'] = df['full_address'].str.replace(r,' ',regex=False)\n",
    "\n",
    "# Remove excess whitespace\n",
    "df[\"business_name\"] = df[\"business_name\"].str.replace(r\" +\",\" \",regex=True)\n",
    "df['full_address'] = df['full_address'].str.replace(r\" +\",\" \",regex=True)\n",
    "\n",
    "# Standardise postal codes - just remove empty space and make sure it's all lower case\n",
    "df.loc[~df.postal_code.isnull(),'postal_code'] = df.loc[~df.postal_code.isnull(),'postal_code'].str.replace(' ','').str.lower()\n",
    "\n",
    "# Some records have street number and street name, but no address field filled\n",
    "df.loc[(df.full_address.isnull())&\\\n",
    "       (~df.formatted_en.isnull()),'full_address']\\\n",
    "    = clean(df.loc[(df.full_address.isnull())&\\\n",
    "       (~df.formatted_en.isnull()),'street_no']+' '+\\\n",
    "           df.loc[(df.full_address.isnull())&\\\n",
    "       (~df.formatted_en.isnull()),'formatted_en']+' '+\\\n",
    "        df.loc[(df.full_address.isnull())&\\\n",
    "       (~df.formatted_en.isnull()),'city'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:12:22.237025Z",
     "iopub.status.busy": "2023-02-16T04:12:22.236865Z",
     "iopub.status.idle": "2023-02-16T04:12:22.996151Z",
     "shell.execute_reply": "2023-02-16T04:12:22.995434Z",
     "shell.execute_reply.started": "2023-02-16T04:12:22.237009Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number removed with business name = NA:  29111\n",
      "Number removed with inactive status:  58598\n"
     ]
    }
   ],
   "source": [
    "# FILTERING\n",
    "\n",
    "# Remove those with no business name\n",
    "\n",
    "print('Number removed with business name = NA: ', len(df[df.business_name.isnull()]))\n",
    "df = df[~df.business_name.isnull()]\n",
    "\n",
    "# Remove those with inactive status\n",
    "no_removed = len(df[df.status.isin(['Gone Out of Business', 'Inactive', 'Cancelled'])])\n",
    "\n",
    "print('Number removed with inactive status: ', no_removed)\n",
    "df = df[~df.status.isin(['Gone Out of Business', 'Inactive', 'Cancelled'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:12:22.997462Z",
     "iopub.status.busy": "2023-02-16T04:12:22.997296Z",
     "iopub.status.idle": "2023-02-16T04:12:23.061161Z",
     "shell.execute_reply": "2023-02-16T04:12:23.060408Z",
     "shell.execute_reply.started": "2023-02-16T04:12:22.997447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_input = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:12:23.062173Z",
     "iopub.status.busy": "2023-02-16T04:12:23.061979Z",
     "iopub.status.idle": "2023-02-16T04:12:23.474149Z",
     "shell.execute_reply": "2023-02-16T04:12:23.473396Z",
     "shell.execute_reply.started": "2023-02-16T04:12:23.062156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LABEL DUPLICATES (NEW)\n",
    "\n",
    "# 1. deduplicate and filter strictly on name, street number and street name\n",
    "df['dupe_1'] = df.duplicated(subset=['business_name', 'street_no', 'formatted_en'], keep=False)\n",
    "\n",
    "df =  df[df['dupe_1'] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:12:23.475153Z",
     "iopub.status.busy": "2023-02-16T04:12:23.474945Z",
     "iopub.status.idle": "2023-02-16T04:12:23.690807Z",
     "shell.execute_reply": "2023-02-16T04:12:23.690091Z",
     "shell.execute_reply.started": "2023-02-16T04:12:23.475128Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sort by street number, then street name, then business name\n",
    "df = df.sort_values(['business_name', 'street_no', 'formatted_en'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:12:23.691816Z",
     "iopub.status.busy": "2023-02-16T04:12:23.691622Z",
     "iopub.status.idle": "2023-02-16T04:12:23.695487Z",
     "shell.execute_reply": "2023-02-16T04:12:23.694955Z",
     "shell.execute_reply.started": "2023-02-16T04:12:23.691800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112098"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:17:04.989232Z",
     "iopub.status.busy": "2023-02-16T04:17:04.988482Z",
     "iopub.status.idle": "2023-02-16T04:18:33.430562Z",
     "shell.execute_reply": "2023-02-16T04:18:33.429983Z",
     "shell.execute_reply.started": "2023-02-16T04:17:04.989213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop and compare duplicates between rows\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # check if the street number is the same as the previous row\n",
    "    if (i > 1):\n",
    "        if (df.at[df.index[i],'business_name'] == df.at[df.index[i-1],'business_name']):\n",
    "            # then street name\n",
    "            if (df.at[df.index[i],'formatted_en'] == df.at[df.index[i-1],'formatted_en']):\n",
    "                # then name\n",
    "                if (df.at[df.index[i],'street_no'] == df.at[df.index[i-1],'street_no']):\n",
    "                    # ok, now we have our potential duplicates\n",
    "                    # if we match all other variables, then we label duplicate\n",
    "                    # if \n",
    "                    \n",
    "                    # make a small dataframe with just two rows\n",
    "                    df2 = df.iloc[[i, i-1]]\n",
    "                    \n",
    "                    df2 = df2[['business_name','licence_number', 'postal_code', 'formatted_en', 'business_sector', 'business_description', 'licence_type', 'primary_NAICS']]\n",
    "\n",
    "                    \n",
    "                    # remove columns which have NA for either value\n",
    "                    df2 = df2.dropna(axis=1)\n",
    "                    \n",
    "                    \n",
    "                    # check if the rows are identical\n",
    "                    if ((df2.iloc[0] == df2.iloc[1]).all()):\n",
    "                        # duplicate\n",
    "                        df.at[df.index[i], 'dupe'] = True\n",
    "                        df.at[df.index[i-1], 'dupe'] = True\n",
    "                    else:    \n",
    "                        df.at[df.index[i], 'dupe'] = False\n",
    "                        # not duplicate\n",
    "                    \n",
    "                    \n",
    "#                     # then province\n",
    "#                     if (df.at[i, 'province'] == df.at[i-1, 'province']):\n",
    "#                         # we might also want to check the source is the same\n",
    "                        \n",
    "#                         # if they are all the same and we have an x,\n",
    "#                         # then assign it to the previous row\n",
    "                        \n",
    "#                         # check if previous value had an 'x' \n",
    "#                         x = df.at[i - 1, 'x']\n",
    "#                         if (not pd.isna(x) and x != 0):\n",
    "#                             # if yes, then assign it to the current row\n",
    "#                             df.at[i, 'x'] = x\n",
    "                            \n",
    "#                             # do the same for y\n",
    "#                             y = df.at[i - 1, 'y']\n",
    "#                             if (not pd.isna(y) and y != 0):\n",
    "#                                 df.at[i, 'y'] = y\n",
    "                    \n",
    "#                             # also add info other columns from the matching stage\n",
    "#                             df.at[i, 'matches_r'] = df.at[i - 1, 'matches_r']\n",
    "#                             df.at[i, 'ratio'] = df.at[i - 1, 'ratio']\n",
    "#                             df.at[i, 'csdname_oda'] = df.at[i - 1, 'csdname_oda']\n",
    "#                             df.at[i, 'keep_match'] = df.at[i - 1, 'keep_match']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:18:33.431751Z",
     "iopub.status.busy": "2023-02-16T04:18:33.431591Z",
     "iopub.status.idle": "2023-02-16T04:18:33.439636Z",
     "shell.execute_reply": "2023-02-16T04:18:33.439095Z",
     "shell.execute_reply.started": "2023-02-16T04:18:33.431736Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    50795\n",
       "True     20689\n",
       "Name: dupe, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dupe'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:14:22.658229Z",
     "iopub.status.busy": "2023-02-16T04:14:22.657980Z",
     "iopub.status.idle": "2023-02-16T04:14:22.666125Z",
     "shell.execute_reply": "2023-02-16T04:14:22.665602Z",
     "shell.execute_reply.started": "2023-02-16T04:14:22.658212Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50924"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dupe'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:19:01.508190Z",
     "iopub.status.busy": "2023-02-16T04:19:01.507954Z",
     "iopub.status.idle": "2023-02-16T04:19:01.529268Z",
     "shell.execute_reply": "2023-02-16T04:19:01.528583Z",
     "shell.execute_reply.started": "2023-02-16T04:19:01.508173Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dup = df[df['dupe'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:19:14.470378Z",
     "iopub.status.busy": "2023-02-16T04:19:14.470138Z",
     "iopub.status.idle": "2023-02-16T04:19:14.777806Z",
     "shell.execute_reply": "2023-02-16T04:19:14.777207Z",
     "shell.execute_reply.started": "2023-02-16T04:19:14.470362Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dup.to_csv('outputs/dupe.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "# mitosheet.sheet(df[(df.dupe_name == False) & (df.dupe_license == False) & (df.dupe_postal == True) & (df.dupe_st_num == True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T04:02:04.357795Z",
     "iopub.status.busy": "2023-02-16T04:02:04.357524Z",
     "iopub.status.idle": "2023-02-16T04:02:04.632337Z",
     "shell.execute_reply": "2023-02-16T04:02:04.631768Z",
     "shell.execute_reply.started": "2023-02-16T04:02:04.357776Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule 1:  112098\n",
      "Rule 2:  0\n",
      "Rule 3:  0\n"
     ]
    }
   ],
   "source": [
    "# LABEL DUPLICATES (OLD)\n",
    "\n",
    "# Rule 1: exact match for business name, license number, street name, postal code - keep one record\n",
    "df['dupe_1'] = df.duplicated(subset=['business_name', 'street_no', 'formatted_en'], keep=False)\n",
    "# df['dupe_1'] = df.duplicated(subset=['business_name','licence_number', 'postal_code', 'formatted_en', 'business_sector', 'business_description', 'licence_type', 'primary_NAICS'], keep=False)\n",
    "print('Rule 1: ', len(df[df['dupe_1'] == True]))\n",
    "# Results: all duplicate\n",
    "\n",
    "# Rule 2: matching licence number\n",
    "# Add - match street name\n",
    "df['dupe_2'] = df.duplicated(subset=['licence_number', 'formatted_en'], keep=False)\n",
    "df.loc[df.licence_number.isna(), 'dupe_2'] = False\n",
    "print('Rule 2: ', len(df[(df.dupe_1 == False) & (df.dupe_2 == True)]))\n",
    "\n",
    "# Rule 3: business name (within datasets)\n",
    "df['dupe_3'] = df.duplicated(subset=['business_name', 'street_no', 'formatted_en'], keep=False)\n",
    "df.loc[df.business_name.isna(), 'dupe_3'] = False\n",
    "print('Rule 3: ', len(df[(df.dupe_1 == False) & (df.dupe_2 == False) & (df.dupe_3 == True)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE AND REMOVE DUPLICATES\n",
    "\n",
    "# Choose which records to keep and which to remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE DUPLICATES\n",
    "len(df['du'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T02:32:06.321212Z",
     "iopub.status.busy": "2023-02-10T02:32:06.320802Z",
     "iopub.status.idle": "2023-02-10T02:32:06.324688Z",
     "shell.execute_reply": "2023-02-10T02:32:06.324247Z",
     "shell.execute_reply.started": "2023-02-10T02:32:06.321195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471255"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label potential duplications\n",
    "len(df)\n",
    "\n",
    "\n",
    "# df_dup_1 = df.loc[df.dupe_1==True]\n",
    "# df_remove = df.duplicated(subset=['business_name','licence_number', 'postal_code', 'street_no'],keep='last')\n",
    "# df.loc[df.dupe==True].to_csv('outputs/simple_dupes.csv',encoding='utf-8')\n",
    "# df = df.drop_duplicates(subset=['business_name','full_address','CSDUID', 'licence_number'],keep='last')\n",
    "# print('No removed with matching name, license, street no and pcode: ', len(df_remove) - len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many are a match, but one record has NA, other has info\n",
    "# That's the key to what I want to do.\n",
    "\n",
    "# I want to remove all the duplicates removed in one stage from the successive rounds. \n",
    "# I could add a condition to say, when I filter to inspect, if it already has a duplicate 1 mark, leave it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T15:32:08.651281Z",
     "iopub.status.busy": "2023-02-07T15:32:08.650983Z",
     "iopub.status.idle": "2023-02-07T15:32:08.867365Z",
     "shell.execute_reply": "2023-02-07T15:32:08.866658Z",
     "shell.execute_reply.started": "2023-02-07T15:32:08.651260Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates found:  63374\n"
     ]
    }
   ],
   "source": [
    "# Rule 2: exact match for business name, license number, street number, postal code - keep one record\n",
    "# Business lisence different\n",
    "\n",
    "df['dupe_2'] = df.duplicated(subset=['business_name', 'postal_code', 'street_no'], keep=False)\n",
    "print('Additional duplicates found: ', len(df[(df.dupe_1 == False) & (df.dupe_2 == True)]))\n",
    "\n",
    "# df_dup_2 = df.loc[df.dupe_2==True]\n",
    "# df_remove = df.duplicated(subset=['business_name', 'postal_code', 'street_no'],keep='last')\n",
    "# print('number duplicated on these that would be removed: ', len(df_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T19:50:11.842625Z",
     "iopub.status.busy": "2023-02-01T19:50:11.842203Z",
     "iopub.status.idle": "2023-02-01T19:50:11.991268Z",
     "shell.execute_reply": "2023-02-01T19:50:11.990395Z",
     "shell.execute_reply.started": "2023-02-01T19:50:11.842579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates found:  13336\n"
     ]
    }
   ],
   "source": [
    "# Rule 3: licence number is not and NA and is duplicated\n",
    "\n",
    "\n",
    "\n",
    "print('Number of duplicates found: ', len(df[df['dupe_3'] == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T17:03:47.791053Z",
     "iopub.status.busy": "2023-02-01T17:03:47.790623Z",
     "iopub.status.idle": "2023-02-01T17:03:47.847729Z",
     "shell.execute_reply": "2023-02-01T17:03:47.846983Z",
     "shell.execute_reply.started": "2023-02-01T17:03:47.791014Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134499"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df.dupe_1 == False) & (df.dupe_2 == False) & (df.dupe_3 == True)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
